# ğŸš¦ Smart Traffic Management System using Reinforcement Learning  

## ğŸ“Œ Overview  
This project implements an **AI-powered Smart Traffic Management System** that uses **Reinforcement Learning (RL)** to optimize traffic signal timings in real-time.  

Instead of fixed timers, the system learns to minimize **vehicle waiting time** and **congestion** by dynamically switching signals based on live traffic density.  

Built using **Google Colab** with **Python, OpenAI Gym, PyTorch, and RL algorithms (Q-learning & DQN)**.  

---

## ğŸ¯ Problem Statement  
Traditional traffic lights work on **fixed timers** that donâ€™t adapt to real-time traffic conditions.  
This leads to:  
- Long waiting times ğŸš—ğŸš•ğŸš™  
- Fuel wastage â›½  
- Increased congestion ğŸš§  

**Goal**: Train an RL agent that observes traffic queues and decides the optimal green signal direction to reduce congestion.  

---

## âš™ï¸ Tech Stack  
- **Python 3.9+**  
- **Google Colab (for training)**  
- **Libraries:**  
  - `gym` â†’ Custom traffic simulation environment  
  - `numpy` â†’ State & reward calculations  
  - `torch` â†’ Deep Q-Network (DQN) implementation  
  - `matplotlib` â†’ Visualizations  

---

## ğŸ— Project Architecture  
smart_traffic_rl/
â”‚â”€â”€ traffic_env.py # Custom Gym environment
â”‚â”€â”€ q_learning.py # Q-learning baseline
â”‚â”€â”€ dqn.py # Deep Q-Network training
â”‚â”€â”€ utils.py # Replay buffer, plotting
â”‚â”€â”€ app.py # (Optional) API wrapper
â”‚â”€â”€ results/ # Training graphs & saved models
â”‚â”€â”€ README.md # Project documentation
â”‚â”€â”€ smart_traffic_rl.ipynb # Colab notebook


---

## ğŸš€ Implementation  

### ğŸ”¹ 1. Environment (OpenAI Gym)  
- State â†’ Queue lengths `[North, South, East, West]`  
- Actions â†’ `0 = NS Green` | `1 = EW Green`  
- Reward â†’ Negative of total waiting cars (minimize congestion)  

### ğŸ”¹ 2. Algorithms Implemented  
1. **Fixed Timer Baseline** (for comparison)  
2. **Q-Learning** (tabular RL)  
3. **Deep Q-Network (DQN)** using PyTorch  

### ğŸ”¹ 3. Training  
- Episodes: `5000+`  
- Exploration: Îµ-greedy strategy  
- Replay Buffer + Target Network for stability  

---

## ğŸ“Š Results  

### âœ… Average Waiting Time Comparison  
| Method        | Avg. Waiting Time (cars/step) | Improvement |
|---------------|-------------------------------|-------------|
| Fixed Timer   | ~14                           | - |
| Q-Learning    | ~10                           | ~28% |
| DQN (PyTorch) | ~9                            | ~35% |

ğŸ“‰ RL-based control significantly **reduced congestion vs fixed-timer** signals.  

*(Graphs available in `/results`)*  

---

